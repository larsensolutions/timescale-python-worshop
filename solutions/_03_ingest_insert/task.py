# solutions/_03_ingest_insert/task.py
"""
This solution improves upon the basic row-by-row INSERT approach by constructing
a single INSERT statement for each batch of generated data, significantly reducing
the number of database round-trips and improving ingestion performance.
Additionally, it reuses the same database connection and cursor for each batch,
which further optimizes the process.
"""


import datetime as dt
import io  # Needed to handle the CSV string
import csv  # Needed to parse the CSV string into rows

from utils.db import get_connection

# Assuming generate_csv_lines_batch is your generator function
from utils.generator import generate_csv_lines_batch
from utils.decorators import time_execution


def build_insert_query(rows):
    """
    Constructs a single INSERT query for multiple rows with ON CONFLICT DO NOTHING.
    """
    if not rows:
        return None

    value_strings = []
    for time_str, id_str, value_str in rows:
        # Quote the time and use standard literal for id and value
        # Note: Time strings generated by isoformat() are safe to quote directly.
        value_strings.append(f"('{time_str}', {id_str}, {value_str})")

    values_clause = ",\n".join(value_strings)

    query = f"""
        INSERT INTO sensors (time, id, value)
        VALUES
        {values_clause}
        ON CONFLICT DO NOTHING;
    """
    return query


@time_execution()
def ingest_insert_commit_solution(cur, sql_query):
    """
    This function is only split from ingest_insert_solution() for timing purposes.
    """
    cur.execute(sql_query)


def ingest_insert_solution():
    for csv_block in generate_csv_lines_batch(
        devices=2,
        step_sec=1,
        batch_size=15000,
        start=dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=100),
        end=dt.datetime.now(dt.timezone.utc),
        drift_per_day=0.01,
        jitter_frac=0.3,
    ):

        num_lines = len(csv_block.splitlines())
        print(f"\nðŸ§¬ Generated {num_lines} lines of sample data")

        with get_connection() as conn, conn.cursor() as cur:
            csv_reader = csv.reader(io.StringIO(csv_block))
            rows = list(
                csv_reader
            )  # List of lists/tuples: [('time', 'id', 'value'), ...]

            sql_query = build_insert_query(rows)

            if sql_query:
                ingest_insert_commit_solution(cur, sql_query)
                print(f"ðŸ“¦ Ingested ~{cur.rowcount} rows.\n")

            conn.commit()


@time_execution()
def run():
    """Run the ingestion task."""
    ingest_insert_solution()


if __name__ == "__main__":
    run()
